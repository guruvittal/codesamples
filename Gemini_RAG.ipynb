{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Leveraging Gemini-Pro for Retrieval Augmented Generation"
      ],
      "metadata": {
        "id": "05-Bo_-JNHGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table align=\"left\"  width=\"100%\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/guruvittal/codesamples/blob/main/Embeddings_Demo.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/guruvittal/codesamples/blob/main/Embeddings_Demo.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/guruvittal/codesamples/main/Embeddings_Demo.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "TBPT1M5pM9pD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies"
      ],
      "metadata": {
        "id": "eeObbnA4NQFv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGH8tjIklgg2"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade google-cloud-aiplatform\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Install langchain and related libraries\n",
        "!pip install langchain unstructured[pdf]\n"
      ],
      "metadata": {
        "id": "PpavwORgtZ65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Using Google Cloud Storage Directory loader from langchain\n",
        "from langchain.document_loaders import GCSDirectoryLoader"
      ],
      "metadata": {
        "id": "Y7Y0J02xtnsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store docs in local vectorstore as index\n",
        "!pip install -q chromadb\n"
      ],
      "metadata": {
        "id": "Mr02oWRCwXuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Authentication"
      ],
      "metadata": {
        "id": "4OVzQEsRNUG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "  from google.colab import auth as google_auth\n",
        "  google_auth.authenticate_user()"
      ],
      "metadata": {
        "id": "bhvaQxF-tDb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constants & Helper Functions"
      ],
      "metadata": {
        "id": "eGTskFkuNXr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GCP\n",
        "PROJECT_ID = \"Project_id\"   # @param {type: \"string\"}\n",
        "LOCATION = 'us-central1' # @param {type: \"string\"}\n",
        "\n",
        "\n",
        "import vertexai\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "from vertexai.preview.generative_models import GenerativeModel, Part\n",
        "\n",
        "def generate():\n",
        "  model = GenerativeModel(\"gemini-pro-vision\")\n",
        "  responses = model.generate_content(\n",
        "    \"\"\"Answer the question: Who is the killer of John?\n",
        "Based on the context: John died due to heart attack\"\"\",\n",
        "    generation_config={\n",
        "        \"max_output_tokens\": 2048,\n",
        "        \"temperature\": 0.9,\n",
        "        \"top_p\": 1\n",
        "    },\n",
        "    safety_settings=[],\n",
        "  stream=True,\n",
        "  )\n",
        "\n",
        "  for response in responses:\n",
        "      print(response.text, end=\"\")\n",
        "\n",
        "print(\"Calling generate\")\n",
        "generate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brUQs6hflh54",
        "outputId": "afeca908-5068-4d0f-ca4a-d391519ff61d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling generate\n",
            "The provided context does not mention anything about a killer, therefore I cannot answer this question."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load documents"
      ],
      "metadata": {
        "id": "wpfQiNgkNjov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = GCSDirectoryLoader(project_name=PROJECT_ID, bucket=\"empdocs\")\n",
        "documents = loader.load()\n",
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-zkoHGIts94",
        "outputId": "9f8e86e7-29d5-4b19-b5fa-5c02d27ccf14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split the documents into chunks\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "print(f\"# of documents = {len(docs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ttJkNRxut7C",
        "outputId": "5f2f0f11-b11f-44dd-f83c-8bd1c4ef9dcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of documents = 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build embeddings for the document corpus"
      ],
      "metadata": {
        "id": "_5H2YzC_N2xh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import VertexAIEmbeddings\n",
        "REQUESTS_PER_MINUTE = 590\n",
        "\n",
        "embedding = VertexAIEmbeddings(model_name=\"textembedding-gecko@001\",requests_per_minute=REQUESTS_PER_MINUTE)\n"
      ],
      "metadata": {
        "id": "NReilE4quxdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Chroma DB as Vector Store Database\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "emphandbook_db = Chroma.from_documents(docs, embedding)"
      ],
      "metadata": {
        "id": "CSHJ1huCwFcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the retrieval algorithm and the neighbors needed"
      ],
      "metadata": {
        "id": "7CeqaH7zN8d6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expose index to the retriever\n",
        "retriever = emphandbook_db.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\":6})"
      ],
      "metadata": {
        "id": "Un9FyVQVwo32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the retrieval Q&A chain"
      ],
      "metadata": {
        "id": "VbZpkOTGOFEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import VertexAI\n",
        "# Create chain to answer questions\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "llm = VertexAI(\n",
        "    model_name='gemini-pro',\n",
        "    max_output_tokens=256,\n",
        "    temperature=0.1,\n",
        "    top_p=0.8,\n",
        "    top_k=40,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# Uses LLM to synthesize results from the search index.\n",
        "# We use Vertex PaLM Text API for LLM\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True)\n"
      ],
      "metadata": {
        "id": "y_3KE8gywyi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test out the retrieval Q&A chain"
      ],
      "metadata": {
        "id": "xv6ZMyDFNvK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Think through the steps before you answer this question: How many days of vacation does an employee get\"\n",
        "result = qa({\"query\": query})\n",
        "print(result[\"query\"])\n",
        "print(result[\"result\"])\n",
        "for i in result[\"source_documents\"]:\n",
        "  print (i.page_content)\n",
        "  print (i.metadata[\"source\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "uJZ2OVhTxETu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir(result.values)"
      ],
      "metadata": {
        "id": "iJu2NkZ7ysbO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}